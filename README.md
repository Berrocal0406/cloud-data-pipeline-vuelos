# Cloud Data Pipeline ‚Äì An√°lisis de Datos de Vuelos ‚úàÔ∏è‚òÅÔ∏è

Pipeline de datos **tipo Cloud Data Engineering** ejecutado localmente, siguiendo el patr√≥n **Medallion (Raw ‚Üí Silver ‚Üí Gold)**.

- **Raw**: datos crudos descargados (CSV sin headers)
- **Silver**: datos limpios y estandarizados (CSV)
- **Gold**: m√©tricas listas para an√°lisis/BI (Parquet)

![Arquitectura](docs/images/medallion_architecture.png)

> En un escenario real: **Raw/Silver** vivir√≠an en un Data Lake (**S3 / ADLS**) y **Gold** se consultar√≠a desde un motor anal√≠tico (**Databricks SQL / Snowflake**) para dashboards.

---

## ‚úÖ Qu√© resuelve este proyecto

1. Ingesta de datos crudos de rutas a√©reas (dataset p√∫blico).
2. Limpieza con **Python + Pandas** (estructurar columnas, tipos y validaciones).
3. Transformaci√≥n y agregaciones con **PySpark** (generar dataset Gold y m√©tricas).
4. Consulta anal√≠tica con **DuckDB (SQL)** sobre el Parquet final.
5. Estructura de repo y scripts reproducibles tipo "pipeline".

---

## üß± Arquitectura (simulaci√≥n Cloud)

```text
data/raw    (CSV crudo)
   ‚Üì  src/clean (Pandas)
data/silver (CSV limpio)
   ‚Üì  src/transform (PySpark)
data/gold   (Parquet listo para anal√≠tica)
   ‚Üì  src/analyze (DuckDB/SQL)
Resultados / m√©tricas para BI
```

**Mapping a Cloud real:**
- `data/raw` ‚Üí **S3 / ADLS Gen2** (landing/raw)
- `data/silver` ‚Üí **S3/ADLS** (curated/clean)
- `data/gold` ‚Üí **Databricks Delta / Snowflake tables**
- `src/transform` ‚Üí **Spark en Databricks / EMR**
- `src/analyze` ‚Üí **Databricks SQL / Snowflake / Athena**

---

## üìÅ Estructura del repositorio

```text
cloud-data-pipeline-vuelos/
  data/
    raw/
    silver/
    gold/
  src/
    utils/
    ingest/
    clean/
    transform/
    analyze/
  sql/
  docs/
```
![EstructuraP](docs/images/project_structure.png)
---

## ‚öôÔ∏è Requisitos

- Windows + PowerShell (recomendado para este repo)
- Python 3.11
- Java 17 (para Spark)
- Dependencias Python (requirements.txt)

---

## üöÄ C√≥mo ejecutar

### 1) Crear y activar entorno

```powershell
py -3.11 -m venv .\.venv
.\.venv\Scripts\python.exe -m pip install --upgrade pip
.\.venv\Scripts\python.exe -m pip install -r requirements.txt
```

> Nota: En Windows, Spark suele requerir variables de entorno para evitar que busque `python3`.
> Usa el script de ejecuci√≥n que viene en el repo (ver siguiente secci√≥n).

---

### 2) Ingesta: descargar dataset a Raw

Guarda el archivo como:

```text
data/raw/routes_raw.csv
```

Dataset sugerido: **OpenFlights Routes** (archivo `routes.dat`).
El dataset raw proviene de OpenFlights, un proyecto p√∫blico que mantiene informaci√≥n abierta sobre aeropuertos, aerol√≠neas y rutas a√©reas.

- El archivo no trae headers. Se renombra a `routes_raw.csv`.
![Raw](docs/images/raw_vuelos.png)

---

### 3) Limpieza (Raw ‚Üí Silver) con Pandas

```powershell
.\.venv\Scripts\python.exe -m src.clean.clean_routes
```

Salida:
- `data/silver/routes_silver.csv`

En Silver, cada fila representa una ruta individual, algo as√≠ como:

airline | source_airport | dest_airport | stops | equipment

En Silver:
- Quitamos nulos.
- Arreglamos tipos.
- Normalizamos valores.

---

### 4) Transformaci√≥n (Silver ‚Üí Gold) con PySpark

```powershell
.\.venv\Scripts\python.exe -m src.transform.transform_routes
```

Salida:
- `data/gold/routes_gold.parquet/` (carpeta Parquet con `part-...` y `_SUCCESS`)

En Gold:
- Cambiamos la forma del dataset.
- Cambiamos el nivel de granularidad.
- Creamos m√©tricas.

Creamos nuevas columnas espec√≠ficas:

source_airport	| dest_airport |	routes_count	| direct_routes_count |	avg_stops

Buscando eliminar ruido para responder: ¬øCu√°les son las rutas m√°s frecuentes y qu√© tan directas son?

---

### 5) Validaci√≥n r√°pida del Parquet

```powershell
.\.venv\Scripts\python.exe -m src.transform.check_gold_parquet
```

---

### 6) An√°lisis con SQL (DuckDB)

```powershell
.\.venv\Scripts\python.exe -m src.analyze.analyze_gold_duckdb
```
![SalidaFinal](docs/images/sql_output_duckbd.png)

Ejemplos de outputs:
- Top rutas por cantidad de vuelos (routes_count)
- Porcentaje aproximado de rutas directas

---

## üß™ Notas de Windows + Spark

- Los warnings sobre `winutils.exe` son comunes en Windows.
- Se mitig√≥ configurando ejecuci√≥n y variables para que Spark use el Python del `.venv`.
- El dataset Gold es **Parquet**, formato t√≠pico de Data Lakes.

---

## Licencia

Este repositorio puede publicarse como **MIT** para permitir uso libre (recomendado para portafolio).

---

## Autor

**Berrocal0406**  
Proyecto de portafolio orientado a **Cloud Data Engineering** (Python, SQL, PySpark).
